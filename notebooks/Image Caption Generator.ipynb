{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe844ad-f913-41c3-a65b-ce16958af5ee",
   "metadata": {},
   "source": [
    "# Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74832238-7c04-4032-9360-217b0d2a6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra \n",
    "import numpy as np \n",
    "# data processing, CSV file I / O (e.g. pd.read_csv) \n",
    "import pandas as pd \n",
    "import os \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate, BatchNormalization, Input\n",
    "from tensorflow.keras.layers import add,TextVectorization,Bidirectional \n",
    "from tensorflow.keras.utils import to_categorical, plot_model \n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input \n",
    "import matplotlib.pyplot as plt # for plotting data \n",
    "import cv2 \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb161a-bb9f-403d-b6c1-1e46a84da6ff",
   "metadata": {},
   "source": [
    "### Reading the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c316f27e-e04c-437b-832e-4d124659e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the image captions\n",
    "def read_file(path):\n",
    "    with open(path,'r') as file:\n",
    "        return file.read().split('\\n')\n",
    "\n",
    "data = read_file('../data/raw/captions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "682c21fb-efe9-4655-a17a-a9150b46105d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22eed10a-97d7-453c-bcf2-625e5fc28eea",
   "metadata": {},
   "source": [
    "### Creating a dictionary of Images and it's captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44134e66-6cef-4bcf-9b15-fc4895dbdb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fef20f-a749-4173-b727-a0a0cfe33ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting image_captions data into dict where keys = images and value = captions\n",
    "def get_data_dictionary(data):\n",
    "    descriptions = dict()\n",
    "    for line in data:\n",
    "        token = line.split('\\t')\n",
    "        new_token = token[0].split('.')\n",
    "        if len(new_token)<2:\n",
    "            continue\n",
    "        image_name  = new_token[0] \n",
    "        caption = new_token[1].split(',')[1]\n",
    "        if image_name in descriptions.keys():\n",
    "            descriptions[image_name].append(caption)\n",
    "        else:\n",
    "            descriptions[image_name] = [caption]\n",
    "    return descriptions\n",
    "\n",
    "descriptions = get_data_dictionary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8485ef43-2761-4bff-9920-b51f630bfdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd1df713-6d3a-4de1-a079-5979dfcaa4c9",
   "metadata": {},
   "source": [
    "### Clean the descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d029f1-9083-4fd4-b446-b058e581b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_punc(text) :\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "def to_lower_case(text) :\n",
    "    return text.lower()\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "def remove_stopwords(text) :\n",
    "    text_words = [word for word in text.split() if ((word not in stopwords_list) and (len(word) > 2))]\n",
    "    text = ' '.join(text_words)\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text) :\n",
    "    return re.sub(r'[0-9]','',text)\n",
    "\n",
    "def remove_multiple_spaces(text) :\n",
    "    return re.sub(' +',' ',text).strip()\n",
    "\n",
    "# gathering all the text cleaning functions in one fuction\n",
    "def clean_text(text) :\n",
    "    text = remove_punc(text)\n",
    "    text = to_lower_case(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    return text\n",
    "\n",
    "# using the above clean_text function to clean captions\n",
    "def clean_captions(descriptions) :\n",
    "    for image in descriptions.keys() :\n",
    "        for index , caption in enumerate(descriptions[image]) :\n",
    "            descriptions[image][index] = clean_text(caption)\n",
    "    return descriptions\n",
    "\n",
    "descriptions = clean_captions(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cda07-44da-4c0d-93d4-3aadf036ac6e",
   "metadata": {},
   "source": [
    "### Saving the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51032c56-f2ad-4458-8ae4-edb3d750d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data dictionary into external file\n",
    "def write_file(path,data) :\n",
    "    lines = []\n",
    "    for image in data.keys() :\n",
    "        for caption in data[image] :\n",
    "            lines.append(image +'\\t'+ caption)\n",
    "    lines = '\\n'.join(lines)\n",
    "    with open(path,'w') as file :\n",
    "        file.write(lines)\n",
    "\n",
    "write_file('../data/processed/cleaned_data.txt', descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f7417-8efa-48d1-92ac-922dc786d2c5",
   "metadata": {},
   "source": [
    "### Importing VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7af509-b39d-4e69-a4f1-5951436e253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing VGG16 model without the output layer\n",
    "features_extractor = VGG16()\n",
    "features_extractor = Model(inputs = features_extractor.inputs , outputs = features_extractor.layers[-2].output)\n",
    "# getting image name list from Images folder location\n",
    "images_path = '../data/raw/Images'\n",
    "images_names = os.listdir(images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ee3c4-512f-4181-b39d-1926ee32276c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "192f515a-b570-4f0f-8b51-94569257ad65",
   "metadata": {},
   "source": [
    "### Using the pre-trained VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41732612-f5d9-4f35-a2ba-2a22ae3bc7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01ecab6c-ff69-47a6-8a2c-81d1ba099d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the extracted VGG16 model to extract images features and build dict where key: images_names and values: images_features\n",
    "def preprocess_image(model, images_path, images_list) :\n",
    "    features = {}\n",
    "    for img in images_list :\n",
    "        path = os.path.join(images_path,img)\n",
    "        image = Image.open(path)\n",
    "        image = image.resize((224,224))\n",
    "        image = np.expand_dims(image,axis = 0)\n",
    "        image = image /127.5\n",
    "        image = image -1\n",
    "        feature = model.predict(image,verbose = 0)\n",
    "        features[img.split('.')[0]] = feature\n",
    "    return features\n",
    "\n",
    "features = preprocess_image(features_extractor, images_path, images_names)\n",
    "\n",
    "# saving images_features dict into .bin file\n",
    "pickle.dump(features, open('../data/processed/image_features.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75a804-e63f-45ee-b2b9-333895854e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "820c5879-0100-45f0-84d6-660afd170098",
   "metadata": {},
   "source": [
    "### Adding tokens in captions and splitting them in train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "198011f2-ccb8-4368-a428-1e9fa546dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# adding startseq and endseq to each caption\n",
    "def load_tokens(path,images) :\n",
    "    lines = read_file(path)\n",
    "    tokens = {}\n",
    "    for line in lines :\n",
    "        img , caption = line.split('\\t')\n",
    "        if img not in tokens.keys() :\n",
    "            tokens[img] = []\n",
    "        tokens[img].append(\"startseq \"+ caption +\" endseq\")\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# listing all available images\n",
    "def list_images(path) :\n",
    "    all_images = []\n",
    "    lines = read_file(path)\n",
    "    for line in lines:\n",
    "        img, caption = line.split('\\t')\n",
    "        if img not in all_images :\n",
    "            all_images.append(img)\n",
    "    return all_images\n",
    "    \n",
    "all_images_list = list_images('../data/processed/cleaned_data.txt')\n",
    "\n",
    "# splitting images to training and testing\n",
    "training_images, testing_images = train_test_split(all_images_list, test_size =0.1, shuffle = True)\n",
    "cross_validation_images , testing_images = train_test_split(testing_images, test_size = 0.5, shuffle = True)\n",
    "\n",
    "# saving the training, validation & testing images_lists\n",
    "pickle.dump(training_images, open('../data/processed/training_images.txt','wb'))\n",
    "pickle.dump(cross_validation_images, open('../data/processed/cross_validation_images.txt','wb'))\n",
    "pickle.dump(testing_images, open('../data/processed/testing_images.txt','wb'))\n",
    "\n",
    "#loading training images_captions dict\n",
    "training_tokens = load_tokens('../data/processed/cleaned_data.txt',training_images)\n",
    "\n",
    "# loading extracted images features\n",
    "features = pickle.load(open('../data/processed/image_features.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f43e1-c1d3-4db9-9f76-6467d2a0e780",
   "metadata": {},
   "source": [
    "### Vectorizing the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a214057-ec92-4715-a8b7-8c53e1c923de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all captions into one list\n",
    "def fetch_captions(tokens) :\n",
    "    captions = []\n",
    "    for caps in tokens.values() :\n",
    "        [captions.append(cap) for cap in caps]\n",
    "    return captions\n",
    "\n",
    "captions = fetch_captions(training_tokens)\n",
    "\n",
    "# searching for captions max_length\n",
    "sentences_length = []\n",
    "for caption in captions :\n",
    "    sentences_length.append(len(caption.split()))\n",
    "\n",
    "max_length = max(sentences_length)\n",
    "\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(captions)\n",
    "\n",
    "# preparing TextVectorization layer to be used to tokenize captions\n",
    "vectorize_layer = TextVectorization(output_mode = 'int')\n",
    "vectorize_layer.adapt(text_dataset)\n",
    "\n",
    "# building vocab using TextVectorization layer\n",
    "vocabulary = list(vectorize_layer.get_vocabulary())\n",
    "vocab_size = vectorize_layer.vocabulary_size()\n",
    "\n",
    "# tokenizing captions and saving it back to dict where keys:images and values:sequences\n",
    "\n",
    "for img , captions in training_tokens.items() :\n",
    "    training_tokens[img] = []\n",
    "    for caption in captions :\n",
    "        sequence = vectorize_layer(tf.constant([caption])).numpy().tolist()[0]\n",
    "        training_tokens[img].append(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa54741-6996-480c-ac08-e7fcbbb2ae91",
   "metadata": {},
   "source": [
    "### Building Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e16ecf9-542f-4a74-ad7e-05ce5b94172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(tokens_keys, tokens,features, vocab_size,max_length,batch_size) :\n",
    "    input_1, input_2, output = [] , [] , []\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for img in tokens_keys :\n",
    "            sequences = tokens[img]\n",
    "            n += 1\n",
    "            if img in features.keys() :\n",
    "                feature = features[img][0]\n",
    "                for sequence in sequences :\n",
    "                    for index in range(1, len(sequence)) :\n",
    "                        input_b = sequence[:index]\n",
    "                        input_b = pad_sequences([input_b],maxlen = max_length, padding = 'post')[0]\n",
    "                        output_w = sequence[index]\n",
    "                        output_w = to_categorical([output_w],num_classes = vocab_size)[0]\n",
    "                        input_1.append(feature)\n",
    "                        input_2.append(input_b)\n",
    "                        output.append(output_w)\n",
    "\n",
    "            if n == batch_size :\n",
    "                try :\n",
    "                    input_1, input_2, output = np.array(input_1), np.array(input_2), np.array(output)\n",
    "                    yield [input_1,input_2], output\n",
    "                    input_1, input_2, output = [], [], []\n",
    "                    n = 0\n",
    "                except :\n",
    "                    print(\"Skipped\")\n",
    "                    input_1, input_2, output = [], [], []\n",
    "                    n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeace54-7c5f-40d3-86e0-8fc4d9c107a6",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52abbf77-8fb3-43e8-ac68-301b8bbf4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no of features ectracted from last layer of VGG16_extractor\n",
    "no_of_features = 4096\n",
    "\n",
    "def build_model(no_of_features, max_length, output_size, learning_rate) :\n",
    "\n",
    " #  images features model path\n",
    "    input_img = Input(shape = (no_of_features,))\n",
    "    cnn_layer1 = Dropout(0.4)(input_img)\n",
    "    cnn_layer2 = Dense(256, activation = 'relu')(cnn_layer1)\n",
    "\n",
    " #  sequences path \n",
    "    input_seq = Input(shape = (max_length,))\n",
    "    lstm_layer1 = Embedding(output_size,300,input_length = max_length, mask_zero = True)(input_seq)\n",
    "    lstm_layer2 = Dropout(0.4)(lstm_layer1)\n",
    "    lstm_layer3 = LSTM(256,activation = 'tanh')(lstm_layer2)\n",
    "\n",
    " #  merging the two model\n",
    "    merging_layer = add([cnn_layer2,lstm_layer3])\n",
    "    final_dense = Dense(256, activation = 'relu')(merging_layer)\n",
    "    output = Dense(output_size , activation = 'softmax')(final_dense)\n",
    "\n",
    " # initiating a model \n",
    "    model = Model(inputs = [input_img, input_seq], outputs = output)\n",
    "\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer = optimizer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "captioning_model = build_model(no_of_features,max_length, vocab_size, learning_rate= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1d211-21b6-470b-9079-bfb157e25c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62dfe778-6fc3-445b-9797-44713361df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 22)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 22, 300)      2530500     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 22, 300)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          1048832     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 256)          570368      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256)          65792       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 8435)         2167795     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,383,287\n",
      "Trainable params: 6,383,287\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "captioning_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87610fbb-d35c-4574-b93b-d097b964afa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbf003fb-9346-483d-b9e0-5bd3d5c84b6c",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49769c82-f16c-45d0-afa0-4b05cd6f5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Define a custom callback class to track the traning loss history\n",
    "class LossHistory(tf.keras.callbacks.Callback) :    \n",
    "# Define a function to initialize the loss history list at the begining of training\n",
    "    def on_train_begin(self, logs={}) :\n",
    "        self.losses = []\n",
    "\n",
    "# Define a function to append the training loss at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}) :\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "history = LossHistory()\n",
    "model_loss = []\n",
    "steps = len(training_tokens) /64\n",
    "\n",
    "for i in range(50) :\n",
    "# shuffling training data before each epoch\n",
    "    tokens_keys = list(training_tokens.keys())\n",
    "    random.shuffle(tokens_keys)\n",
    "    data = data_generator(tokens_keys, training_tokens, features, vocab_size,max_length,64)\n",
    "\n",
    "    captioning_model.fit(data, epochs = 1, steps_per_epoch = steps, verbose = 0, callbacks =[history])\n",
    "\n",
    "# extracting epoch model loss and saving it into txt file\n",
    "    loss = history.losses\n",
    "    model_loss.append([loss[0]])\n",
    "\n",
    "# saving the model \n",
    "captioning_model.save('../models/model_1.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36a079-7fd4-4ae4-9b2b-d0d2b5c37a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b870dff-3fe7-4329-8d18-d1f6216266f8",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f657e61-0297-4547-9a6e-de0d6412016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_image(image_path,model) :\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224,224))\n",
    "    img = np.expand_dims(img, axis= 0)\n",
    "    img = img/127.5\n",
    "    img = img -1\n",
    "    features = model.predict(img,verbose = 0)\n",
    "    return features\n",
    "\n",
    "def get_word(index,vocab) :\n",
    "    word = vocab[index]\n",
    "    return word\n",
    "\n",
    "def get_caption(path, features_extractor, vectorize_layer, captioning_model) :\n",
    "    my_features = get_features_from_image(path,features_extractor)\n",
    "    caption = 'startseq'\n",
    "    for i in range(max_length) :\n",
    "        sequenced_caption = vectorize_layer(tf.constant([caption])).numpy().tolist()\n",
    "        padded_sequenced_caption = pad_sequences(sequenced_caption, maxlen = max_length, padding = 'post')[0]\n",
    "        padded_sequenced_caption = np.resize(padded_sequenced_caption,(1,max_length))\n",
    "        output = captioning_model.predict([my_features , padded_sequenced_caption],verbose = 0)\n",
    "        index = np.argmax(output)\n",
    "        if index == 2:\n",
    "            caption = caption + ' endseq'\n",
    "            return caption\n",
    "        else:\n",
    "            current_word = get_word(index,vocabulary)\n",
    "            caption = caption + ' ' + current_word\n",
    "    return caption\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588f66c-1de0-4791-a3f7-ed99d1c268be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluation_func(captioning_model, vectorize_layer, features_extractor, images_folder, images_set_path) :\n",
    "    images_names = read_file(images_set_path)\n",
    "    images_tokens = load_tokens('../data/processed/cleaned_data.txt',images_names)\n",
    "    actual, predicted = list(), list()\n",
    "    for image in images_tokens.keys() :\n",
    "        image_path = os.path.join(images_folder,image+'.jpg')\n",
    "        generated_caption = get_caption(image_path,features_extractor,vectorize_layer,captioning_model)\n",
    "        actual_captions = images_tokens[image]\n",
    "        actual.append([caption.split() for caption in actual_captions])\n",
    "        predicted.append(generated_caption.split())\n",
    "\n",
    "    BLEU_1 = corpus_bleu(actual,predicted,weights = (1.0,0,0,0))\n",
    "    return BLEU\n",
    "\n",
    "training_bleu_score = evaluation_func(captioning_model, vectorize_layer,features_extractor, images_path,\n",
    "                                     '../data/processed/training_images.txt')\n",
    "\n",
    "validation_bleu_score = evaluation_func(captioning_model, vectorize_layer,features_extractor, images_path,\n",
    "                                        '../data/processed/cross_validation_images.txt')\n",
    "\n",
    "testing_bleu_score = evaluation_func(captioning_model, vectorize_layer,features_extractor, images_path,\n",
    "                                     '../data/processed/testing_images.txt')\n",
    "\n",
    "print(f'Training BLEU score    : {training_bleu_score}')\n",
    "print(f'Validation BLEU score  : {validation_bleu_score}')\n",
    "print(f'Testing BLEU score     : {testing_bleu_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadcf09d-4d79-494f-a087-6d7c2dc3c1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b000930c-7800-400b-acf7-848b0524a946",
   "metadata": {},
   "source": [
    "### Visualizing the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "445a06c3-e1f1-48ac-a338-8babd22be94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting top k probabilities and indexes\n",
    "def get_word_preds(sentence, testing_image, beam_size) :\n",
    "    sequenced_caption = vectorize.layer(tf.constant([sentence])).numpy().tolist()\n",
    "    padded_sequenced_caption = pad_sequences(sequenced_caption, maxlen=max_length, padding='post')[0]\n",
    "    padded_sequenced_caption = np.resize(padded_sequenced_caption, (1,max_length))\n",
    "    preds = captioning_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bed1b0-23b0-4a99-bb27-396eb5da09b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "12327d29-bdf4-4ec9-a12a-9111c965fafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b9416-abf2-4c56-a644-c427b74ebd16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
